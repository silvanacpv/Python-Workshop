{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69aae9f-03a3-4169-ae3e-e99f81e3e519",
   "metadata": {},
   "source": [
    "# Data Analytics and Visualization (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834dde3c-30de-4f77-ac17-871aeb695e62",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5503f3e-fe4f-47f2-8bc4-cac33bfd1c5c",
   "metadata": {},
   "source": [
    "Data Cleaning involves identifying and rectifying errors, inconsistencies, and inaccuracies within the dataset. By eliminating missing values, outliers, and redundant information, data quality is enhanced, leading to more accurate and reliable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59b90d-fe4d-4f75-88d6-fa2a898a664d",
   "metadata": {},
   "source": [
    "### Handling Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71baa96a-da42-4a12-802e-10617ed4a265",
   "metadata": {},
   "source": [
    "Missing values in a dataset can hinder analysis and modeling. Pandas provides functions to handle missing values, such as  **fillna()**, which allows us to fill ***NaN*** values with a specific value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec77051-48c4-4e1a-aa77-7924324b805c",
   "metadata": {},
   "source": [
    "Let's first start by importing our libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a672e8-ec35-40cc-80c8-53bc5456ca4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import pandas and numpy libraries\n",
    "\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded1384-6e75-4ce5-85ee-fbf066212fdb",
   "metadata": {},
   "source": [
    "Now, let's practice filling ***NaN*** values using **fillna()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0ea1a2e-44bb-4ef6-b819-e73e9afd4427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame with missing values\n",
    "data = { \n",
    "     'A':[1, 2, None, 4, 5],\n",
    "     'B':[10, 20, 30, 40, 50]}\n",
    "#Your code goes here\n",
    "\n",
    "# Before filling missing value\n",
    "#Your code goes here\n",
    "\n",
    "# Filling missing values with 0\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2edac-e0a9-4ef9-89f3-b7d2a58f6eb6",
   "metadata": {},
   "source": [
    "**[Question 2: Complete the 'handle_missing_values' function]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98fe26-62ba-4291-8b12-2d1f7b2b2c7e",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d49cf-aaf4-41f7-8fc5-194ce1b48b12",
   "metadata": {},
   "source": [
    "Outliers are extreme values that can skew analysis and modeling results. Pandas can help us identify and handle outliers. In this example, we identify outliers using the **interquartile range (IQR)** method and remove them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b7a57-1998-402e-9beb-f1543ca54ce3",
   "metadata": {},
   "source": [
    "The **Interquartile Range (IQR)** is a measure of statistical dispersion, representing the range within which the middle 50% of the data lies. It is calculated as the difference between the 75th percentile (also called the third quartile, or Q3) and the 25th percentile (the first quartile, or Q1) of a dataset.\n",
    "\n",
    "The IQR is useful in identifying the spread of data, and it is commonly used to detect outliers. Values that are significantly lower than Q1 or significantly higher than Q3 are often considered outliers.\n",
    "\n",
    "Here's a breakdown of what the IQR represents in pandas and how to compute it:\n",
    "\n",
    "**What IQR Represents**\n",
    "-  Q1 (25th percentile): The value below which 25% of the data falls.\n",
    "-  Q3 (75th percentile): The value below which 75% of the data falls.\n",
    "-  IQR: The range between Q1 and Q3, calculated as IQR = Q3 - Q1.\n",
    "\n",
    "**How to Calculate IQR in pandas:**\n",
    "To calculate the IQR for a specific column in a pandas DataFrame, you can use the quantile method to get the 25th and 75th percentiles, and then subtract them to find the IQR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4230219-938b-48f7-a959-09df92495705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame with outliers\n",
    "data = {\n",
    "        'A' : [1, 2, 3, 4, 5], \n",
    "        'B' : [10, 20, 30, 200, 50]\n",
    "}\n",
    "#Your code goes here\n",
    "\n",
    "# Main Dataframe with Outliers\n",
    "#Your code goes here\n",
    "\n",
    "# Identifying and handling outliers\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982fefd-3853-40be-b726-861467479fff",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "-  **q1** and **q3** are calculated using the **quantile()** function, representing the first and third quartiles of column ‘B’.\n",
    "-  **q1** represents the value below which 25% of the data lies. For column ‘B’, **q1** would be the median of the first half of the sorted values, which is 15.\n",
    "-  **q3** represents the value below which 75% of the data lies. For column ‘B’, **q3** would be the median of the second half of the sorted values, which is 50.\n",
    "-  **iqr** (Interquartile Range) is computed as the difference between **q3** and **q1**.\n",
    "-  **lower_bound** and **upper_bound** are calculated to define the thresholds beyond which data points are considered outliers. These bounds are defined as 1.5 times the IQR below q1 and above q3.\n",
    "-  The line **df_no_outliers = df[(df['B'] >= lower_bound) & (df['B'] <= upper_bound)]** filters the DataFrame to keep only the rows where the values in column ‘B’ fall within the acceptable range, effectively removing the outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7dbfc-190d-495e-8490-36bbb62df51b",
   "metadata": {},
   "source": [
    "**[Question 3: Complete the 'handle_outliers' function]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30162c8-2de2-40fe-80e9-059c6f9825c8",
   "metadata": {},
   "source": [
    "### Dealing with Duplicate Data\n",
    "Duplicate data can lead to misleading analysis. Pandas provides functions to detect and remove duplicate rows. Here’s how we can do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65a0f465-712e-461b-ab21-758ff097cf0a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame with duplicate data\n",
    "data = {\n",
    "        'A': [1,2,2,3,4,4],\n",
    "        'B': [10,20,20,30,40,40]\n",
    "}\n",
    "#Your code goes here\n",
    "\n",
    "# Main DataFrame with duplicate data\n",
    "#Your code goes here\n",
    "\n",
    "# Detecting and removing duplicated rows\n",
    "#Your code goes here\n",
    "\n",
    "#Detecting and removing duplicated rows but keeping the first duplicate or the last duplicate\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aac6de-a8c7-4187-b7fe-8d7c078c5d6e",
   "metadata": {},
   "source": [
    "**[Question 4: Complete the 'handle_duplicates' function]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79085465-c240-43e9-abca-826c67c31a7f",
   "metadata": {},
   "source": [
    "### Data Reshaping\n",
    "Reshaping data is the process of transforming data from one format to another. In the context of data analysis and machine learning (ML), reshaping data often involves reorganizing it into a different structure that is better suited for analysis, visualization, or modeling. Reshaping can involve tasks such as pivoting, melting, stacking, unstacking, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d4205-2b6d-43ed-bdf8-ae31f69d461d",
   "metadata": {},
   "source": [
    "#### Wide to Long Format (Melting)\n",
    "In this transformation, we convert a dataset from a wide format (many columns) to a long format (fewer columns) by melting or unpivoting it. This is useful when we have variables stored as columns and we want to gather them into a single column.\n",
    "\n",
    "Melting data is useful for making it more suitable for analysis, especially when we want to compare or aggregate across different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80194ee0-ee89-4f3a-9dbd-f707a7a79074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a Wide DataFrame\n",
    "data = {\n",
    "        'ID': [1,2,3],\n",
    "        'Math': [90,85,78],\n",
    "        'Science':[75,88,92]\n",
    "}\n",
    "\n",
    "#Your code goes here\n",
    "\n",
    "# Main Wide DataFrame\n",
    "#Your code goes here\n",
    "\n",
    "# Melting the DataFrame\n",
    "#Your code goes here\n",
    "\n",
    "# After Melting the DataFrame\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf7b0e-276c-4462-9436-2bc545313fc5",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "-  The **pd.melt()** function is used to transform the *df* DataFrame from wide format to long format.\n",
    "-  **id_vars=['ID']** specifies that the ‘ID’ column should be kept as an identifier for each observation.\n",
    "-  **value_vars=['Math', 'Science']** specifies the columns (‘Math’ and ‘Science’) whose values will be “melted” or transformed into a single column.\n",
    "-  **var_name='Subject'** specifies the name of the new column that will store the subject names (‘Math’ and ‘Science’).\n",
    "-  **value_name='Score'** specifies the name of the new column that will store the scores for each subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e66fd-1dc1-4e9e-b878-c93cbfa71810",
   "metadata": {},
   "source": [
    "#### Long to Wide Format (Pivoting)\n",
    "This transformation involves converting a long-format dataset back into a wide format by pivoting or spreading the values.\n",
    "\n",
    "Pivoting is useful when we want to reshape data to make it easier to visualize or perform calculations on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dda1eb1-017d-49ad-bd69-27e5437c5815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a Long DataFrame\n",
    "data = {\n",
    "        'ID': [1,1,2,2],\n",
    "        'Subject': ['Math','Science','Math','Science'],\n",
    "        'Score': [90,75,85,88]\n",
    "}\n",
    "#Your code goes here\n",
    "\n",
    "# Main Long DataFrame\n",
    "#Your code goes here\n",
    "\n",
    "# Pivoting the DataFrame\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17881c55-4323-4c3a-b45f-357ef2dc60b2",
   "metadata": {},
   "source": [
    "**Code Explanation** \n",
    "-  The **df_long.pivot()** function is used to transform the df_long DataFrame from a long format to a wide format.\n",
    "-  **index='ID'** specifies that the ‘ID’ column will be the index of the resulting pivoted DataFrame.\n",
    "-  **columns='Subject'** specifies that the unique values in the ‘Subject’ column will become the column headers of the pivoted DataFrame.\n",
    "-  **values='Score'** specifies that the values in the ‘Score’ column will be placed in the corresponding cells of the pivoted DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13195248-d6b9-49b8-b2a4-42447a84dffa",
   "metadata": {},
   "source": [
    "### Stacking and Unstacking\n",
    "Stacking involves converting columns into rows, and unstacking is the reverse process. These operations can be useful for creating hierarchical indexes and dealing with multi-level data.\n",
    "\n",
    "Stacking and unstacking can make data manipulation and analysis easier when dealing with multi-indexed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9acf6f-9afe-4c98-a748-5b829271c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "data = {\n",
    "    'ID':[1,2],\n",
    "    'Math':[90,85],\n",
    "    'Science':[75,88]\n",
    "}\n",
    "\n",
    "#Your code goes here\n",
    "\n",
    "# Original DataFrame\n",
    "#Your code goes here\n",
    "\n",
    "# Set the DataFrame index using the ID column\n",
    "#Your code goes here\n",
    "\n",
    "# Doing Stacking and Unstacking\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1c3fb5-b06a-4a59-8b3f-8bf9fd6fb488",
   "metadata": {},
   "source": [
    "### Handling Inconsistent Data and Standardizing\n",
    "Handling inconsistent data is a crucial step in data preprocessing to ensure the accuracy and reliability of our analysis or modeling. Inconsistent data refers to values that do not adhere to the expected format or constraints. This can include typos, varying representations, or unexpected values in categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23a7fe-0621-4aec-a2b8-c8453a8a6ad0",
   "metadata": {},
   "source": [
    "Suppose we have a dataset with a “Location” column that contains variations of the categories. To handle inconsistencies, we can standardize the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a815113-5787-4e5c-87c9-3e5efd09a146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 6],\n",
    "    'Location': ['toronto', ' ToRonto', ' VANCOUVER', 'calgary ', 'CALGARY', 'vancouver']\n",
    "}\n",
    "#Your code goes here\n",
    "\n",
    "# Original DataFrame\n",
    "#Your code goes here\n",
    "\n",
    "# Convert gender values to lowercase and standardize\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c80b98-9249-4cde-8e3b-cb982e6d1cc4",
   "metadata": {},
   "source": [
    "**Code Explanation**\n",
    "-  **df['Location']** selects the ‘Location’ column from the DataFrame.\n",
    "-  **.str.lower()** is a string method that converts all the values in the ‘Location’ column to lowercase. This ensures that all variations of the cities are in lowercase, making the replacement consistent.\n",
    "-  **replace({'toronto': 'Toronto', 'vancouver': 'Vancouver', 'calgary':'Calgary'})** is used to replace specific values in the ‘Location’ column.\n",
    "    - This replacement is case-insensitive due to the prior conversion to lowercase. For instance, ‘toronto’ and ‘toRonto’ will both be converted to ‘Toronto’.\n",
    "-  **.str.strip()** helps in removing leading and trailing whitespaces from a string. When dealing with textual data in Python, especially from external sources like files or user input, it's common to encounter unwanted leading or trailing whitespaces. These spaces might seem harmless, but they can significantly impact data analysis, leading to inconsistencies and errors.\n",
    "-  The updated ‘Location’ column, after performing the lowercase conversion and replacements, is assigned back to the original ‘Location’ column in the DataFrame. This effectively updates the values in the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f54861d-1de0-4dfa-b4ff-dab900976899",
   "metadata": {},
   "source": [
    "Now, suppose we have a dataset with a “Color” column that contains various color names, including some inconsistent spellings and synonyms. We want to standardize these color names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0b697b-7466-4a08-ba7b-bc97249283b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame\n",
    "data = {\n",
    "    'ID': [1,2,3,4,5],\n",
    "    'Color': ['red', 'green', 'blue', 'Green', 'Reddish']\n",
    "\n",
    "}\n",
    "#Your code goes here\n",
    "\n",
    "# Original DataFrame\n",
    "#Your code goes here\n",
    "\n",
    "# Define a mapping for inconsistent color names to standard names\n",
    "#Your code goes here\n",
    "\n",
    "# Apply the mapping to the Color column\n",
    "#Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175de4f-911b-4ec1-9c61-682f18c4d365",
   "metadata": {},
   "source": [
    " **[Question 5 & 6: Complete the 'standardize_data' function and save your cleaned dataframe]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
